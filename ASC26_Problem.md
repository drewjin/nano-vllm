# SHUSCT-ASC26 队员选拔：AI 赛题

## 背景说明

本赛题旨在考察参赛者对推理框架（Nano-vLLM）的理解、模型集成能力与性能优化能力。评测重点包括：能否在源码层面理解并接入新模型、实现特定架构（例如 MoE）的推理支持，并通过基准测试验证性能与正确性。

## 任务概要

任务分三部分：模型集成、性能验证与性能优化（附加题）。要求在不改动核心设计的前提下，实现可复现、可验证的集成与 Benchmark 流程，并提交完整结果与复现说明。

### 任务一：将模型集成进 Nano-vLLM

- 集成模型：`Mini-Mixtral`（权重：[`Mini-Mixtral-v0.2`](https://huggingface.co/NeuralNovel/Mini-Mixtral-v0.2)）。
- 对照实现：`Mistral-7B`（权重：[`Mistral-7B-v0.2`](https://huggingface.co/unsloth/mistral-7b-v0.2)）。

子任务：
- 在 Nano-vLLM 中完成模型的加载与推理实现，保证能够以相同的输入输出接口运行。
- 若模型为 MoE 架构，需实现或扩展相应的 MoE 推理路径，使其在 Nano-vLLM 中正确运行。

成功标准：能够加载权重并完成一次端到端推理（输出合理，不报错）。

### 任务二：性能验证（Benchmark）

- 基准模型：使用现有的 `Qwen3` 集成作为参考（权重：[`Qwen3-8B`](https://huggingface.co/Qwen/Qwen3-8B)），目的是验证 Benchmark 流程的正确性与可比性。
- 测试数据集：GSM8K（数学任务）与 MBPP（编程任务）。建议使用 LM Eval（或等效工具）编写并运行 Benchmark 脚本。

需要提交：Benchmark 脚本、运行命令、以及每次实验的原始输出（日志）。

成功标准：`Mini-Mixtral` 与 `Mistral-7B` 能在 Nano-vLLM 中完成 Benchmark 并产出合理度量（如准确率/通过率、延迟、吞吐等）。

### 任务三：性能优化（附加题）

在已有实现基础上，对推理速度或资源占用进行优化（例如算子融合、内存布局、并行策略或 MoE-specific 优化）。目标是使 `Mini-Mixtral` 在相同硬件/配置下的速度优于至少两个对照的 Dense 模型（说明优化方法与验证结果）。

## 提交要求

- 将所有评测结果和复现步骤写入 `ASC26_Results.md`（包含环境、依赖、模型与权重链接、数据集版本、Benchmark 命令与结果表、结论与讨论）。

- 请保留原始日志与关键脚本，便于复现。

- 请在完成每一部分子任务或某个关键模块后，即做好git commit，commit message请使用中文，符合格式如下: 
    - (feat): ...; ...; ...
    - (refactor): ...
    - 更详细格式请参考 [Angular提交信息规范](https://zj-git-guide.readthedocs.io/zh-cn/latest/message/Angular%E6%8F%90%E4%BA%A4%E4%BF%A1%E6%81%AF%E8%A7%84%E8%8C%83/)

## 评审与评分标准（简化版）

重要门槛 — 可复现性（必须）：
- 所有提交的 Benchmark 结果必须能够通过文档中提供的一行运行命令在指定环境中复现（包含依赖清单、数据集说明与随机种子）。若评审无法依据提交的说明和命令复现结果，则该提交得分为 0 分（不可复现即 0 分）。


评分按任务对齐，总分 100，前提是通过可复现性门槛：

1) 朴素 MoE 集成 + `Mini-Mixtral` 集成 — 30 分
- 要点：将 `Mini-Mixtral` 与其（若有）朴素 MoE 推理支持一并实现并能够正常运行与推理（包含路由/专家处理的基本正确性）。

2) `Mistral-7B` 集成 — 10 分
- 要点：`Mistral-7B` 能否在 `nano-vllm` 中正确加载与完成端到端推理（输出合理且无报错）。

3) Benchmark 实现与结果提交 — 40 分
- 要点：提供可运行的 Benchmark 脚本（GSM8K、MBPP）、明确的运行命令、原始日志与结果文件。结果必须可复现且与提交的命令一致。

4) 附加题：推理效率/优化 — 20 分
- 要点：对推理速度或资源使用进行优化，并用数据证明改进（例如延迟/吞吐/内存占用的对比）。

合计：30 + 10 + 40 + 20 = 100 分

必需的可复现性清单（提交必须包含，下列任一缺失视为不可复现）：
- 1. 一行可运行的复现命令（示例必须能在干净环境中自动运行）；
- 2. `requirements.txt` 或 pyproject/lockfile；
- 3. 数据集来源与预处理说明 / 脚本；
- 4. 明确的随机种子、样本数量与暖身步骤（warm-up）；
- 5. 产生的结果文件或日志路径（需附上原始日志）；
- 6. 最小 smoke-test（建议在 15 分钟内能完成的短例子）以便评审快速验证。

说明：若提交满足上述可复现性要求，则按上表逐项评分；若不满足，则直接判为不可复现并计 0 分。